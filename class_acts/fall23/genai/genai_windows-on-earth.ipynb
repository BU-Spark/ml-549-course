{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windows on Earth GenAI Kickstarter\n",
    "\n",
    "## Team Members:\n",
    "- Hemant Kumar Singh\n",
    "- Jaisal Singh\n",
    "- Vedika Srivastava\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Document the prompts you attempted and the responses you used.\n",
    "\n",
    "Additional dataset info is at [project_data.ipynb](project_data.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Prompts to Consider\n",
    "\n",
    "1. Retrieve EXIF data from the images to extract metadata like:\n",
    "    1. focal length\n",
    "    2. aperture\n",
    "    3. altitude\n",
    "    4. any other metadata that might help determine the field of view of the camera\n",
    "2. Use the metadata to determine the field of view of the camera\n",
    "3. Explore earth image dataset APIs so that you can specify a location and field of view\n",
    "4. Retrieve images, using the API, for candidate images in the approximate location of the camera\n",
    "5. Explore rotation and scale invariant image comparison algorithms\n",
    "    1. template matching\n",
    "    2. SIFT, e.g. https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/\n",
    "6. Try to apply image matching to the labeled test images\n",
    "\n",
    "See [project desription](https://docs.google.com/document/d/12sIHG_ojL2QaLsC0cHfGQsMW76cg01O8P-ZK6fb-398/edit?usp=sharing)\n",
    "for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copilot/ChatGPT Prompt**\n",
    "\n",
    "1. Retrieve EXIF data from the images to extract metadata like:\n",
    "    1. focal length\n",
    "    2. aperture\n",
    "    3. altitude\n",
    "    4. any other metadata that might help determine the field of view of the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response**\n",
    "\n",
    "*response here and in code cell*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Here\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "\n",
    "def get_exif_data(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        exif_data = img._getexif()\n",
    "        if exif_data:\n",
    "            exif = {}\n",
    "            for tag_id, value in exif_data.items():\n",
    "                tag = TAGS.get(tag_id, tag_id)\n",
    "                exif[tag] = value\n",
    "            return exif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:** Use the metadata to determine the field of view of the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**response:**\n",
    "\n",
    "To determine the field of view of the camera using the metadata, you can use the following formula:\n",
    "\n",
    "```\n",
    "field_of_view = 2 * arctan(sensor_size / (2 * focal_length))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# assume sensor size is 36mm x 24mm (full frame)\n",
    "sensor_size = math.sqrt(36**2 + 24**2)\n",
    "\n",
    "# assume focal length is in mm\n",
    "focal_length = exif_data.get(\"FocalLength\")\n",
    "\n",
    "if focal_length:\n",
    "    field_of_view = 2 * math.atan(sensor_size / (2 * focal_length))\n",
    "    print(f\"Field of view: {math.degrees(field_of_view)} degrees\")\n",
    "else:\n",
    "    print(\"Focal length not found in metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assumes a full-frame sensor size of 36mm x 24mm, but you can adjust it to match the actual sensor size of the camera. The code also assumes that the focal length is in millimeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:** Explore earth image dataset APIs so that you can specify a location and field of view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:** \n",
    "\n",
    "There are several Earth image dataset APIs that allow you to specify a location and field of view. Here are a few examples:\n",
    "\n",
    "1. NASA Earth Observations (NEO) API: This API provides access to satellite imagery and other Earth science data from NASA's Earth Observing System Data and Information System (EOSDIS). You can specify a location and date range to retrieve data from various instruments, including MODIS, Landsat, and VIIRS. The API also provides tools for filtering and processing the data.\n",
    "\n",
    "2. Google Earth Engine API: This API provides access to a vast collection of satellite imagery and other geospatial data, along with tools for processing and analyzing the data. You can specify a location and time range to retrieve data from various sources, including Landsat, Sentinel-2, and MODIS. The API also provides tools for filtering and processing the data.\n",
    "\n",
    "3. Planet API: This API provides access to a large collection of satellite imagery from Planet's constellation of satellites. You can specify a location and time range to retrieve data from various sensors, including PlanetScope, RapidEye, and SkySat. The API also provides tools for filtering and processing the data.\n",
    "\n",
    "4. DigitalGlobe API: This API provides access to high-resolution satellite imagery from DigitalGlobe's constellation of satellites. You can specify a location and time range to retrieve data from various sensors, including WorldView-3, WorldView-4, and GeoEye-1. The API also provides tools for filtering and processing the data.\n",
    "\n",
    "These APIs offer a wide range of capabilities for exploring Earth image datasets, and the choice of API will depend on your specific needs and use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:** Retrieve images, using the API, for candidate images in the approximate location of the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**\n",
    "\n",
    "To retrieve images using an API for candidate images in the approximate location of the camera, you can use the following steps:\n",
    "\n",
    "1. Use the metadata from the image to determine the approximate location of the camera.\n",
    "2. Use the Earth image dataset API to search for images in the vicinity of the camera location.\n",
    "3. Filter the search results based on the field of view of the camera and other criteria, such as image resolution and cloud cover.\n",
    "4. Download the candidate images for further analysis.\n",
    "\n",
    "Here's some Python code that demonstrates how to use the Google Earth Engine API to retrieve images for a given location and field of view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# define the location and field of view\n",
    "lat = 37.7749\n",
    "lon = -122.4194\n",
    "fov = 60  # in degrees\n",
    "\n",
    "# create a point geometry for the camera location\n",
    "point = ee.Geometry.Point(lon, lat)\n",
    "\n",
    "# create a bounding box geometry for the field of view\n",
    "bbox = ee.Geometry.Rectangle([lon - fov/2, lat - fov/2, lon + fov/2, lat + fov/2])\n",
    "\n",
    "# search for Landsat 8 images in the vicinity of the camera location\n",
    "collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR') \\\n",
    "    .filterBounds(point) \\\n",
    "    .filterDate('2019-01-01', '2020-01-01') \\\n",
    "    .filter(ee.Filter.lt('CLOUD_COVER', 10)) \\\n",
    "    .filter(ee.Filter.intersects('.geo', bbox))\n",
    "\n",
    "# select the first image in the collection\n",
    "image = ee.Image(collection.first())\n",
    "\n",
    "# download the image to a file\n",
    "url = image.getDownloadURL({\n",
    "    'scale': 30,\n",
    "    'crs': 'EPSG:4326',\n",
    "    'region': bbox.toGeoJSONString()\n",
    "})\n",
    "print(f\"Download URL: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code searches for Landsat 8 images in the vicinity of the camera location, filters the results based on cloud cover and field of view, and selects the first image in the collection. It then downloads the image to a file using the Earth Engine API. You can modify the code to use a different Earth image dataset API and adjust the search criteria to match your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:**\n",
    "\n",
    "Explore rotation and scale invariant image comparison algorithms\n",
    "    1. template matching\n",
    "    2. SIFT, e.g. https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**\n",
    "\n",
    "Sure, here's some information on rotation and scale invariant image comparison algorithms:\n",
    "\n",
    "1. Template matching: This is a simple and intuitive method for comparing images that involves sliding a template image over a larger search image and computing a similarity score at each position. The similarity score can be based on pixel intensity values or other image features. Template matching is sensitive to changes in rotation and scale, so it may not be suitable for all applications.\n",
    "\n",
    "2. Scale-Invariant Feature Transform (SIFT): This is a popular method for comparing images that is based on detecting and matching local features that are invariant to scale, rotation, and affine distortion. SIFT works by identifying keypoints in an image and computing descriptors that capture the local appearance and orientation of the keypoints. The descriptors are then matched between pairs of images to determine their similarity. SIFT is widely used in computer vision applications such as object recognition, image stitching, and 3D reconstruction.\n",
    "\n",
    "Here's some Python code that demonstrates how to use the `cv2` library to perform template matching and SIFT feature matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# load the template and search images\n",
    "template = cv2.imread('template.png', cv2.IMREAD_GRAYSCALE)\n",
    "search = cv2.imread('search.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# perform template matching\n",
    "result = cv2.matchTemplate(search, template, cv2.TM_CCOEFF_NORMED)\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "print(f\"Template matching score: {max_val}\")\n",
    "\n",
    "# perform SIFT feature matching\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(template, None)\n",
    "kp2, des2 = sift.detectAndCompute(search, None)\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "print(f\"SIFT matching score: {len(good_matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads a template image and a search image, and performs template matching and SIFT feature matching between the two images. The template matching score and SIFT matching score are printed to the console. You can modify the code to use your own images and adjust the matching parameters to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:** Try to apply image matching to the labeled test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response:**\n",
    "\n",
    "Sure, here's an example of how to apply image matching to the labeled test images using the SIFT feature matching algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load the labeled test images\n",
    "img1 = cv2.imread('test_images/label_1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('test_images/label_2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# initialize the SIFT feature detector and descriptor\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# detect keypoints and compute descriptors for the labeled test images\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# initialize the brute-force matcher\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# match the descriptors between the labeled test images\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# apply ratio test to filter out false matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# draw the matched keypoints on the labeled test images\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# display the result\n",
    "cv2.imshow('Matched Keypoints', img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads the labeled test images, detects keypoints and computes descriptors using the SIFT feature detector and descriptor, matches the descriptors between the images using the brute-force matcher, and applies the ratio test to filter out false matches. It then draws the matched keypoints on the labeled test images and displays the result. You can modify the code to use your own test images and adjust the matching parameters to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
