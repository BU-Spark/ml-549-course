{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS/DS 549 Spring 2023 Programming and Model Training Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/BU-Spark/ml-549-course/blob/main/homeworks/spring24_hw1/sp24_hw1_pt.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.png\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to train an image classifier on the CIFAR10\n",
    "dataset. CIFAR10 is a \"toy\" dataset in that it only has 10 classes and image\n",
    "resolution of 32x32x3. There are 50K training images and 10K test images.\n",
    "\n",
    "Your task is to define a _neural network model_ and _training\n",
    "hyperparameters_ to reach a __minimum validation accuracy\n",
    "of 82%__.\n",
    "\n",
    "The code below is fully functional albeit a simple model with basic hyperparameters.\n",
    "Update the model and hyperparameters to get the validation accuracy above the\n",
    "minimum.\n",
    "\n",
    "The parts of the code you need to change are indicated by the delimiters:\n",
    "\n",
    "```python\n",
    "### BEGIN SOLUTION\n",
    "<your code>\n",
    "### END SOLUTION\n",
    "```\n",
    "\n",
    "Aspects you may want to experiment with are:\n",
    "1. Deeper and more complex models.\n",
    "2. Different training optimizers.\n",
    "3. Hyperparameters like batch_size and epochs.\n",
    "\n",
    "If you really want get more sophisticated, but not necessary, you can attempt\n",
    "1. pre-training on a larger dataset and transfer learning\n",
    "2. additional input image augmentations.\n",
    "\n",
    "The following are **prohibited**, however:\n",
    "1. Loading a predefined model.\n",
    "2. Using a pre-trained model.\n",
    "\n",
    "You can run this notebook locally on your computer, on SCC or on Google Colab,\n",
    "with the proper setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install here for Colab\n",
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "batch_size = 4\n",
    "### END SOLUTION\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some statistics on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The size of the training set is {len(trainset)} images\")\n",
    "print(f\"The size of the test set is {len(testset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of trainloader is \", len(trainloader))\n",
    "print(\"The size of testloader is \", len(testloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why is trainloader and testloader smaller than trainset and testset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__### Put your answer here. ###__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the neural network layers we want to use in our model's\n",
    "        forward pass.\"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(6 * 15 * 15, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)    # we have to end with 10 outputs because we have 10 classes\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 6 * 15 * 15)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        ### END SOLUTION\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "print(summary(net, input_size=(batch_size, 3, 32, 32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize a list to hold the losses\n",
    "losses = []\n",
    "\n",
    "# Calculate number of batches to process to report 10 times per epoch\n",
    "len_trainloader = len(trainloader)\n",
    "report_every = len_trainloader // 10\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "epochs = 2\n",
    "### END SOLUTION\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record the loss\n",
    "        # losses.append(loss.item())\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % report_every == report_every - 1:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / report_every))\n",
    "\n",
    "            losses.append(running_loss / report_every)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Epoch {} took {} seconds'.format(epoch+1, end_time - start_time))\n",
    "\n",
    "print('Finished Training with training loss of ', losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Batch # / 2000')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy on the training data\n",
    "correct = 0\n",
    "total = 0\n",
    "train_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        outputs = net(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_train_loss = train_loss / len(trainloader)\n",
    "\n",
    "print('Accuracy of the network on the training images: %d %%' % (accuracy))\n",
    "print('Average training loss: %.3f' % (avg_train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set the baseline accuracy we want for this exercise.\n",
    "baseline_accuracy = 80.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        outputs = net(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / len(testloader)\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (accuracy))\n",
    "print('Average test loss: %.3f' % (avg_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('And did you beat the baseline accuracy of ', baseline_accuracy, '?')\n",
    "\n",
    "if accuracy > baseline_accuracy:\n",
    "    print('Yes!!! You beat the baseline accuracy!')\n",
    "else:\n",
    "    print('Awww! Try again! You can do it!')\n",
    "\n",
    "# We put an assert here to raise an error if the accuracy is not greater than the baseline accuracy\n",
    "assert accuracy > baseline_accuracy, 'Accuracy did not beat baseline accuracy of {}'.format(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What was the accuracy of the model as initially defined in the assginment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__### YOUR ANSWER HERE ###__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Try running the same model and training hyperparameters multiple times. Does the loss curve and final accuracy differ between runs? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__### YOUR ANSWER HERE ###__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Your Experiments Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful if you track your experiments so you can see what you changed and how it impacted accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Template (Copy and Edit)\n",
    "\n",
    "**Model Definition:**\n",
    "```python\n",
    "<model definition here if changed>\n",
    "```\n",
    "\n",
    "**Hyperparameters:**\n",
    "```python\n",
    "batch size:\n",
    "epochs:\n",
    "optimizer:\n",
    "learning rate:\n",
    "momentum:\n",
    "```\n",
    "Training Loss:   <br>\n",
    "Training Accuracy: <br>\n",
    "\n",
    "Test Loss:    <br>\n",
    "**Test Accuracy: nn%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
